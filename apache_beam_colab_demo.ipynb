{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samipn/apache-beam-demo/blob/main/apache_beam_colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm0lha7oK4HW"
      },
      "source": [
        "# Apache Beam in Colab — end‑to‑end demo\n",
        "**What you'll learn**\n",
        "\n",
        "- Pipeline IO: `ReadFromText`, `WriteToText`\n",
        "\n",
        "- Elementwise transforms: `Map`, `Filter`, `ParDo`\n",
        "\n",
        "- Partitioning with `beam.Partition`\n",
        "\n",
        "- Windowing with `FixedWindows` using `TestStream` (simulated streaming)\n",
        "\n",
        "- Composite transforms with custom `PTransform`\n",
        "\n",
        "- *(Bonus)* Beam ML `RunInference` with scikit‑learn\n",
        "\n",
        "\n",
        "\n",
        "**How to use**\n",
        "\n",
        "Run the cells from top to bottom. You can re-run individual sections safely.\n",
        "\n",
        "\n",
        "### Useful links\n",
        "- Getting started (interactive): https://beam.apache.org/get-started/an-interactive-overview-of-beam/\n",
        "\n",
        "- Beam Playground: https://beam.apache.org/get-started/try-beam-playground\n",
        "\n",
        "- Primary Beam Colab (official): https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/interactive-overview/getting-started.ipynb\n",
        "\n",
        "- Python transforms (elementwise): https://beam.apache.org/documentation/transforms/python/elementwise/overview/\n",
        "\n",
        "- Windowing concepts: https://beam.apache.org/documentation/programming-guide/#windowing\n",
        "\n",
        "- RunInference (sklearn): https://beam.apache.org/documentation/transforms/python/elementwise/runinference-sklearn/\n",
        "\n",
        "- Beam ML: https://beam.apache.org/documentation/ml/overview/\n",
        "\n"
      ],
      "id": "Gm0lha7oK4HW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrM1qQ_VK4HX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83dc5143-651d-4ca7-8dba-22a4e4472f16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.7/276.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.1/529.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install deps (Colab-friendly)\n",
        "# If you run into version issues, remove version pins or restart runtime.\n",
        "!pip -q install apache-beam[gcp] scikit-learn"
      ],
      "id": "DrM1qQ_VK4HX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3yaSRlVK4HY",
        "outputId": "c8da62b3-854c-4a43-99c3-f48b6d1b5d96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam version: 2.68.0\n"
          ]
        }
      ],
      "source": [
        "# Imports & sanity checks\n",
        "import os, re, json, random, string, textwrap, time\n",
        "import numpy as np\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.transforms import window as beam_window\n",
        "\n",
        "print(\"Beam version:\", beam.__version__)"
      ],
      "id": "o3yaSRlVK4HY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1ZToRWRK4HY",
        "outputId": "40b1a876-3250-4380-e6aa-6bad56bbac68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be, or not to be: that is the Question.\n",
            "Whether 'tis nobler in the mind to suffer\n",
            "The slings and arrows of outrageous fortune,\n",
            "Or to take arms against a sea of troubles\n",
            "And by opposing end them. To die—to sleep,\n",
            "No more; and by a sleep to say we end\n",
            "The heart-ache and the thousand natural shocks\n",
            "That flesh is heir to: 'tis a consummation\n",
            "Devoutly to be wish'd. To die, to sleep—\n",
            "To sleep—perchance to dream: ay, there's the rub.\n"
          ]
        }
      ],
      "source": [
        "# Create a tiny input dataset for IO demos\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "with open('/content/data/input.txt', 'w') as f:\n",
        "    f.write(\"\"\"To be, or not to be: that is the Question.\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd. To die, to sleep—\n",
        "To sleep—perchance to dream: ay, there's the rub.\n",
        "\"\"\")\n",
        "\n",
        "!head -n 10 /content/data/input.txt"
      ],
      "id": "-1ZToRWRK4HY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjObWraK4HZ"
      },
      "source": [
        "## 1) Batch pipeline: IO + Map/Filter + Combine"
      ],
      "id": "JWjObWraK4HZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "X6MpyhTaK4HZ",
        "outputId": "c5ee3221-befa-4e10-f300-92e5be6a504c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/output/basic_wordcount\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 337 Oct 27 00:38 part-00000-of-00001\n",
            "Top 10 lines:\n",
            "not\t1\n",
            "that\t2\n",
            "the\t6\n",
            "question\t1\n",
            "whether\t1\n",
            "tis\t2\n",
            "nobler\t1\n",
            "mind\t1\n",
            "suffer\t1\n",
            "slings\t1\n"
          ]
        }
      ],
      "source": [
        "# Basic batch pipeline with IO, Map, Filter\n",
        "# - Reads from text\n",
        "# - Cleans and tokenizes\n",
        "# - Filters short tokens\n",
        "# - Counts words\n",
        "# - Writes to text\n",
        "\n",
        "def tokenize(line):\n",
        "    # lowercase + keep letters only, split on non-letters\n",
        "    return [w for w in re.split(r'[^A-Za-z]+', line.lower()) if w]\n",
        "\n",
        "def is_long_enough(word, min_len=3):\n",
        "    return len(word) >= min_len\n",
        "\n",
        "output_dir = '/content/output/basic_wordcount'\n",
        "!rm -rf \"$output_dir\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions()) as p:\n",
        "    counts = (\n",
        "        p\n",
        "        | \"ReadLines\" >> beam.io.ReadFromText('/content/data/input.txt')\n",
        "        | \"Tokenize\" >> beam.FlatMap(tokenize)       # Map that returns iterables\n",
        "        | \"FilterShort\" >> beam.Filter(is_long_enough)\n",
        "        | \"PairWithOne\" >> beam.Map(lambda w: (w, 1)) # Map\n",
        "        | \"Count\" >> beam.CombinePerKey(sum)\n",
        "        | \"Format\" >> beam.Map(lambda kv: f\"{kv[0]}\t{kv[1]}\")\n",
        "    )\n",
        "    counts | \"WriteToText\" >> beam.io.WriteToText(os.path.join(output_dir, 'part'))\n",
        "\n",
        "print('Wrote:', output_dir)\n",
        "!ls -l \"$output_dir\"\n",
        "!echo 'Top 10 lines:'\n",
        "!head -n 10 \"$output_dir\"/part-00000-of-00001"
      ],
      "id": "X6MpyhTaK4HZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnOTbCTPK4HZ"
      },
      "source": [
        "## 2) ParDo (DoFn) + Partition"
      ],
      "id": "pnOTbCTPK4HZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd9StC_8K4HZ",
        "outputId": "d195d997-c125-4bdb-fa6c-b7a6cdae3fcd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample partitioned results:\n",
            "SHORT    not     3\n",
            "SHORT    that     4\n",
            "SHORT    the     3\n",
            "SHORT    tis     3\n",
            "SHORT    mind     4\n",
            "SHORT    and     3\n",
            "SHORT    take     4\n",
            "SHORT    arms     4\n",
            "SHORT    sea     3\n",
            "SHORT    end     3\n",
            "SHORT    them     4\n",
            "SHORT    die     3\n",
            "SHORT    more     4\n",
            "SHORT    say     3\n",
            "SHORT    ache     4\n"
          ]
        }
      ],
      "source": [
        "# ParDo (DoFn) and Partition demo\n",
        "\n",
        "class TagLengthDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        # element is a word; emit a dict with length metadata\n",
        "        result = {\n",
        "            'word': element,\n",
        "            'length': len(element),\n",
        "            'is_long': len(element) >= 6\n",
        "        }\n",
        "        yield result\n",
        "\n",
        "# We will replace the by_length function and beam.Partition with Filters\n",
        "# def by_length(num_partitions, element):\n",
        "#     # 0: short (<5), 1: medium (5-7), 2: long (>=8)\n",
        "#     l = len(element['word'])\n",
        "#     if l < 5:\n",
        "#         return 0\n",
        "#     elif l < 8:\n",
        "#         return 1\n",
        "#     else:\n",
        "#         return 2\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions()) as p:\n",
        "    words = (\n",
        "        p\n",
        "        | beam.io.ReadFromText('/content/data/input.txt')\n",
        "        | beam.FlatMap(tokenize)\n",
        "        | beam.Filter(is_long_enough)\n",
        "    )\n",
        "\n",
        "    tagged = (words | \"TagWithParDo\" >> beam.ParDo(TagLengthDoFn()))\n",
        "    # Convert dict to tuple before Distinct\n",
        "    distinct_tagged_tuple = tagged | \"ToTuple\" >> beam.Map(lambda d: tuple(d.items()))\n",
        "    distinct_tagged = distinct_tagged_tuple | \"Distinct\" >> beam.Distinct()\n",
        "    # Convert tuple back to dict after Distinct\n",
        "    distinct_tagged_dict = distinct_tagged | \"ToDict\" >> beam.Map(dict)\n",
        "\n",
        "    # Replace Partition with Filters\n",
        "    short = distinct_tagged_dict | \"FilterShort\" >> beam.Filter(lambda d: d['length'] < 5)\n",
        "    medium = distinct_tagged_dict | \"FilterMedium\" >> beam.Filter(lambda d: 5 <= d['length'] < 8)\n",
        "    long = distinct_tagged_dict | \"FilterLong\" >> beam.Filter(lambda d: d['length'] >= 8)\n",
        "\n",
        "\n",
        "    short_fmt  = short  | beam.Map(lambda d: f\"SHORT    {d['word']}     {d['length']}\")\n",
        "    medium_fmt = medium | beam.Map(lambda d: f\"MED      {d['word']}     {d['length']}\")\n",
        "    long_fmt   = long   | beam.Map(lambda d: f\"LONG     {d['word']}     {d['length']}\")\n",
        "\n",
        "    all_parts = ((short_fmt, medium_fmt, long_fmt) | beam.Flatten())\n",
        "    all_parts | beam.io.WriteToText('/content/output/partitions/part')\n",
        "\n",
        "!echo 'Sample partitioned results:'\n",
        "!head -n 15 /content/output/partitions/part-00000-of-00001"
      ],
      "id": "zd9StC_8K4HZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipUpya5K4Ha"
      },
      "source": [
        "## 3) Composite transforms (custom PTransform)"
      ],
      "id": "mipUpya5K4Ha"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I8HDjxsK4Hb",
        "outputId": "88409a63-bee9-45ff-e953-7b7ee2ea048a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Composite transform output (first 12 lines):\n",
            "not\t1\n",
            "that\t2\n",
            "question\t1\n",
            "whether\t1\n",
            "tis\t2\n",
            "nobler\t1\n",
            "mind\t1\n",
            "suffer\t1\n",
            "slings\t1\n",
            "arrows\t1\n",
            "outrageous\t1\n",
            "fortune\t1\n"
          ]
        }
      ],
      "source": [
        "# Composite transform (custom PTransform)\n",
        "class CleanTokenize(beam.PTransform):\n",
        "    def __init__(self, min_len=3, stopwords=None):\n",
        "        super().__init__()\n",
        "        self.min_len = min_len\n",
        "        self.stopwords = set(stopwords or [])\n",
        "\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | \"CT:Tokenize\" >> beam.FlatMap(tokenize)\n",
        "            | \"CT:FilterLen\" >> beam.Filter(lambda w: len(w) >= self.min_len)\n",
        "            | \"CT:FilterStop\" >> beam.Filter(lambda w: w not in self.stopwords)\n",
        "        )\n",
        "\n",
        "class CountWords(beam.PTransform):\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | \"CW:PairOne\" >> beam.Map(lambda w: (w, 1))\n",
        "            | \"CW:Count\" >> beam.CombinePerKey(sum)\n",
        "        )\n",
        "\n",
        "STOPWORDS = {'the', 'and', 'to', 'of', 'a', 'in', 'is', 'be'}\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions()) as p:\n",
        "    results = (\n",
        "        p\n",
        "        | beam.io.ReadFromText('/content/data/input.txt')\n",
        "        | CleanTokenize(min_len=3, stopwords=STOPWORDS)\n",
        "        | CountWords()\n",
        "        | beam.Map(lambda kv: f\"{kv[0]}\t{kv[1]}\")\n",
        "        | beam.io.WriteToText('/content/output/composite/part')\n",
        "    )\n",
        "\n",
        "!echo 'Composite transform output (first 12 lines):'\n",
        "!head -n 12 /content/output/composite/part-00000-of-00001"
      ],
      "id": "1I8HDjxsK4Hb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvCGOqRK4Hb"
      },
      "source": [
        "## 4) Windowing with `FixedWindows` via `TestStream`"
      ],
      "id": "VjvCGOqRK4Hb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZEUMxHdK4Hb",
        "outputId": "87f31839-4191-42af-a957-de1f6fb5dca1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windowed results:\n",
            "'part-[0.0, 10.0)-00000-of-00001'  'part-[10.0, 20.0)-00000-of-00001'\n",
            "window[0s-10s)      u1   8\n",
            "window[0s-10s)      u2   7\n",
            "window[10s-20s)      u1   1\n",
            "window[10s-20s)      u2   4\n",
            "window[10s-20s)      u3   6\n"
          ]
        }
      ],
      "source": [
        "# Windowing with TestStream (simulated streaming)\n",
        "# We'll generate (user, amount) events across timestamps and sum per 10-second windows.\n",
        "\n",
        "import apache_beam as beam # Added import\n",
        "from apache_beam.testing.test_stream import TestStream\n",
        "from apache_beam.transforms.window import TimestampedValue # Corrected import\n",
        "from apache_beam.transforms import window as beam_window # Added import\n",
        "\n",
        "def format_window(kv, w=beam.DoFn.WindowParam):\n",
        "    (key, amount) = kv\n",
        "    return f\"window[{int(w.start)}s-{int(w.end)}s)      {key}   {amount}\"\n",
        "\n",
        "# Simulate a tiny stream\n",
        "events = [\n",
        "    TimestampedValue(('u1', 5),  1),\n",
        "    TimestampedValue(('u2', 7),  2),\n",
        "    TimestampedValue(('u1', 3),  8),\n",
        "    TimestampedValue(('u2', 4), 12),  # falls in next window\n",
        "    TimestampedValue(('u3', 6), 15),\n",
        "    TimestampedValue(('u1', 1), 18),\n",
        "]\n",
        "\n",
        "ts = (TestStream()\n",
        "      .advance_watermark_to(0)\n",
        "      .add_elements(events[:3])\n",
        "      .advance_watermark_to(10)\n",
        "      .add_elements(events[3:])\n",
        "      .advance_watermark_to_infinity())\n",
        "\n",
        "out_dir = '/content/output/windowing'\n",
        "!rm -rf \"$out_dir\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions()) as p:\n",
        "    wins = (\n",
        "        p\n",
        "        | \"TestStream\" >> ts\n",
        "        | \"KV\" >> beam.Map(lambda kv: (kv[0], kv[1]))\n",
        "        | \"Window10s\" >> beam.WindowInto(beam_window.FixedWindows(10))\n",
        "        | \"SumPerKey\" >> beam.CombinePerKey(sum)\n",
        "        | \"Fmt\" >> beam.Map(format_window)\n",
        "    )\n",
        "    wins | beam.io.WriteToText(os.path.join(out_dir, 'part'))\n",
        "\n",
        "!echo 'Windowed results:'\n",
        "!ls \"$out_dir\"\n",
        "!cat \"$out_dir\"/part-*"
      ],
      "id": "8ZEUMxHdK4Hb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryploGVuK4Hc"
      },
      "source": [
        "## 5) Bonus: Beam ML RunInference (scikit‑learn)"
      ],
      "id": "ryploGVuK4Hc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY6ZXI_zK4Hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbcda15-73a0-46f1-d7d6-8819053f429e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-a6e10b67-c178-4dde-8ca1-bea519d01daa.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 predictions:\n",
            "pred_label=1\n",
            "pred_label=0\n",
            "pred_label=2\n",
            "pred_label=1\n",
            "pred_label=1\n",
            "pred_label=0\n",
            "pred_label=1\n",
            "pred_label=2\n",
            "pred_label=1\n",
            "pred_label=1\n"
          ]
        }
      ],
      "source": [
        "# (Bonus) Beam ML RunInference with scikit-learn\n",
        "# Train a tiny classifier on Iris and run inference inside a Beam pipeline.\n",
        "# If this section errors due to version mismatches, try restarting runtime and reinstalling Beam.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle # Import pickle instead of joblib\n",
        "import os # Import os for path joining\n",
        "\n",
        "from apache_beam.ml.inference.base import RunInference\n",
        "from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=400).fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file using pickle\n",
        "model_path = '/content/iris_model.pkl' # Change file extension to pkl\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(clf, f)\n",
        "\n",
        "# Pass the model_uri to the handler\n",
        "handler = SklearnModelHandlerNumpy(model_uri=model_path)\n",
        "\n",
        "out_dir = '/content/output/iris_inference'\n",
        "!rm -rf \"$out_dir\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "def fmt_pred(pred):\n",
        "    # pred.inference is typically a numpy scalar or array; make it int\n",
        "    try:\n",
        "        label = int(pred.inference)\n",
        "    except Exception:\n",
        "        # fall back if it's an array\n",
        "        label = int(np.array(pred.inference).item())\n",
        "    return f\"pred_label={label}\"\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions()) as p:\n",
        "    _ = (\n",
        "        p\n",
        "        | beam.Create(X_test.tolist())\n",
        "        | \"Infer\" >> RunInference(handler)\n",
        "        | \"Fmt\" >> beam.Map(fmt_pred)\n",
        "        | beam.io.WriteToText(out_dir + \"/part\")\n",
        "    )\n",
        "\n",
        "!echo 'First 10 predictions:'\n",
        "!head -n 10 \"$out_dir\"/part-00000-of-00001"
      ],
      "id": "UY6ZXI_zK4Hc"
    }
  ]
}